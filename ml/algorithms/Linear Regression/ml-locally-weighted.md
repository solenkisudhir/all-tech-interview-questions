# ML | Locally weighted Linear Regression 

[Linear Regression](https://www.geeksforgeeks.org/ml-linear-regression/) is a supervised learning algorithm used for computing linear relationships between input (X) and output (Y). The steps involved in ordinary linear regression are:

> **Training phase:** Compute ![\theta      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ab5a2374f78ab5ed6496d72845d45086_l3.png "Rendered by QuickLaTeX.com")to minimize the cost. ![J(\theta) = $\sum_{i=1}^{m} (\theta^Tx^{(i)} - y^{(i)})^2    ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e599f60aacb6f67d537a36d7bc71f4c4_l3.png "Rendered by QuickLaTeX.com")
> 
> **Predict output:** for given query point ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com"), ![return: \theta^Tx](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a928a7b822ed5278326c7f87ed0d902e_l3.png "Rendered by QuickLaTeX.com")

![](https://media.geeksforgeeks.org/wp-content/uploads/Linear-Regression-2.png) 

As evident from the image below, this algorithm cannot be used for making predictions when there exists a non-linear relationship between X and Y. In such cases, locally weighted linear regression is used. 

![](https://media.geeksforgeeks.org/wp-content/uploads/Linear-Regression-on-non-linear-data.png)

### Locally Weighted Linear Regression:

Locally weighted linear regression is a non-parametric algorithm, that is, the model does not learn a fixed set of parameters as is done in ordinary linear regression. Rather parameters ![\theta      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ab5a2374f78ab5ed6496d72845d45086_l3.png "Rendered by QuickLaTeX.com")are computed individually for each query point ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com"). While computing ![\theta      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ab5a2374f78ab5ed6496d72845d45086_l3.png "Rendered by QuickLaTeX.com"), a higher “preference” is given to the points in the training set lying in the vicinity of ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com")than the points lying far away from ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com"). The modified cost function is: ![J(\theta) = $\sum_{i=1}^{m} w^{(i)}(\theta^Tx^{(i)} - y^{(i)})^2      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-8e0998ca55c489ff7e715a6e912a72b5_l3.png "Rendered by QuickLaTeX.com")where, ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")is a non-negative “weight” associated with training point ![x^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-71409eb50290c1824d778ca4043557d0_l3.png "Rendered by QuickLaTeX.com"). For ![x^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-71409eb50290c1824d778ca4043557d0_l3.png "Rendered by QuickLaTeX.com")s lying closer to the query point ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com"), the value of ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")is large, while for ![x^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-71409eb50290c1824d778ca4043557d0_l3.png "Rendered by QuickLaTeX.com")s lying far away from ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com")the value of ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")is small.   A typical choice of ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")is: ![w^{(i)} = exp(\frac{-(x^{(i)} - x)^2}{2\tau^2})      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-5d79d604bb3029c42b5e59bce2b39f0a_l3.png "Rendered by QuickLaTeX.com")where ![\tau      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-49aa3721edd7da7cc91922a8ac4b3a9c_l3.png "Rendered by QuickLaTeX.com")is called the bandwidth parameter and controls the rate at which ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")falls with distance from ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com")Clearly, if ![|x^{(i)} - x|      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1b41806b91c596d7de36ce9e85ede6a9_l3.png "Rendered by QuickLaTeX.com")is small ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")is close to 1 and if ![|x^{(i)} - x|      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1b41806b91c596d7de36ce9e85ede6a9_l3.png "Rendered by QuickLaTeX.com")is large ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")is close to 0. Thus, the training set points lying closer to the query point ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com")contribute more to the cost ![J(\theta)      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-831723cc7123967f032b3da0ab39a993_l3.png "Rendered by QuickLaTeX.com")than the points lying far away from ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com"). 

NOTE: For Locally Weighted Linear Regression, the data must always be available on the machine as it doesn’t learn from the whole set of data in a single shot. Whereas, in Linear Regression, after training the model the training set can be erased from the machine as the model has already learned the required parameters.

**For example:** Consider a query point ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com")\= 5.0 and let ![x^{(1)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-3329f508ca6cfc674201da1c5b5c002c_l3.png "Rendered by QuickLaTeX.com")and ![x^{(2)      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-61c4210e724e5cf1bfaf3a3a1d787760_l3.png "Rendered by QuickLaTeX.com")be two points in the training set such that ![x^{(1)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-3329f508ca6cfc674201da1c5b5c002c_l3.png "Rendered by QuickLaTeX.com")\= 4.9 and ![x^{(2)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-0cbda6efde71ab6790eeac8e115c96a9_l3.png "Rendered by QuickLaTeX.com")\= 3.0. Using the formula ![w^{(i)} = exp(\frac{-(x^{(i)} - x)^2}{2\tau^2})      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-5d79d604bb3029c42b5e59bce2b39f0a_l3.png "Rendered by QuickLaTeX.com")with ![\tau      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-49aa3721edd7da7cc91922a8ac4b3a9c_l3.png "Rendered by QuickLaTeX.com")\= 0.5: ![w^{(1)} = exp(\frac{-(4.9 - 5.0)^2}{2(0.5)^2}) = 0.9802      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-6fa8c1df014c28ae65255ad6d12aa7ab_l3.png "Rendered by QuickLaTeX.com")\[Tex\]w^{(2)} = exp(\\frac{-(3.0 – 5.0)^2}{2(0.5)^2}) = 0.000335      \[/Tex\]![So, \ J(\theta) = 0.9802*(\theta^Tx^{(1)} - y^{(1)}) + 0.000335*(\theta^Tx^{(2)} - y^{(2)})      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-35e5605f475b1f974a46d39964156d57_l3.png "Rendered by QuickLaTeX.com")Thus, the weights fall exponentially as the distance between ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com")and ![x^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-71409eb50290c1824d778ca4043557d0_l3.png "Rendered by QuickLaTeX.com")increases and so does the contribution of error in prediction for ![x^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-71409eb50290c1824d778ca4043557d0_l3.png "Rendered by QuickLaTeX.com")to the cost. Consequently, while computing ![\theta      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ab5a2374f78ab5ed6496d72845d45086_l3.png "Rendered by QuickLaTeX.com"), we focus more on reducing ![(\theta^Tx^{(i)} - y^{(i)})^2      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-0537a8f123323032e15c574279308e5a_l3.png "Rendered by QuickLaTeX.com")for the points lying closer to the query point (having a larger value of ![w^{(i)}      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-18edc9c16c5e858d0372d0248662fcab_l3.png "Rendered by QuickLaTeX.com")). 

![](https://media.geeksforgeeks.org/wp-content/uploads/Locally-Weighted-Linear-Regression-1.png) 

**Steps involved in locally weighted linear regression are:**

> **Compute** ![     ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-67781d1e58a588fb103afe1b4ae99dbe_l3.png "Rendered by QuickLaTeX.com")**to minimize the cost.** ![J(\theta) = $\sum_{i=1}^{m} w^{(i)}(\theta^Tx^{(i)} - y^{(i)})^2    ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2fe5dfa6b7480bc4242fda14cbdaef5d_l3.png "Rendered by QuickLaTeX.com")
> 
> **Predict Output:** for given query point ![x      ](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-23c2f21ca9c85759c9bfc2a71c146e77_l3.png "Rendered by QuickLaTeX.com"), ![return: \theta^Tx](https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a928a7b822ed5278326c7f87ed0d902e_l3.png "Rendered by QuickLaTeX.com")

**Points to remember:**

*   Locally weighted linear regression is a supervised learning algorithm.
*   It is a non-parametric algorithm.
*   There exists No training phase. All the work is done during the testing phase/while making predictions.
*   The dataset must always be available for predictions.
*   Locally weighted regression methods are a generalization of k-Nearest Neighbour.
*   In Locally weighted regression an explicit local approximation is constructed from the target function for each query instance.
*   The local approximation is based on the target function of the form like constant, linear, or quadratic functions localized kernel functions.
